{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import numpy as np\n",
    "\n",
    "class WeatherEnricher:\n",
    "    \"\"\"\n",
    "    Enrichit un dataset Uber avec des donn√©es m√©t√©orologiques\n",
    "    Adapt√© pour format: Date='2024-03-23', Time='12:29:38', Location='Uttam Nagar,New Delhi'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        self.geolocator = Nominatim(user_agent=\"uber_weather_ml_project_2024\")\n",
    "        self.cache = {}\n",
    "        self.location_cache = {}\n",
    "    \n",
    "    def categorize_weather(self, weather_code):\n",
    "        \"\"\"\n",
    "        Cat√©gorise selon WMO Weather codes\n",
    "        \"\"\"\n",
    "        weather_mapping = {\n",
    "            0: \"Clear\",\n",
    "            1: \"Partly Cloudy\", 2: \"Partly Cloudy\", 3: \"Partly Cloudy\",\n",
    "            45: \"Foggy\", 48: \"Foggy\",\n",
    "            51: \"Light Rain\", 53: \"Moderate Rain\", 55: \"Heavy Rain\",\n",
    "            61: \"Light Rain\", 63: \"Moderate Rain\", 65: \"Heavy Rain\",\n",
    "            71: \"Snow\", 73: \"Snow\", 75: \"Snow\", 77: \"Snow\",\n",
    "            80: \"Rain Showers\", 81: \"Rain Showers\", 82: \"Heavy Rain Showers\",\n",
    "            85: \"Snow\", 86: \"Snow\",\n",
    "            95: \"Thunderstorm\", 96: \"Thunderstorm\", 99: \"Heavy Thunderstorm\"\n",
    "        }\n",
    "        return weather_mapping.get(weather_code, \"Cloudy\")\n",
    "    \n",
    "    def clean_location_name(self, location):\n",
    "        \"\"\"\n",
    "        Nettoie le nom de localisation pour am√©liorer le g√©ocodage\n",
    "        Ex: 'Uttam Nagar,New Delhi Railway Station' -> 'Uttam Nagar, New Delhi, India'\n",
    "        \"\"\"\n",
    "        if pd.isna(location):\n",
    "            return None\n",
    "        \n",
    "        # S√©parer par virgule\n",
    "        parts = [p.strip() for p in str(location).split(',')]\n",
    "        \n",
    "        # Prendre les premi√®res parties significatives\n",
    "        if len(parts) >= 2:\n",
    "            # Format: \"Quartier, Ville\"\n",
    "            cleaned = f\"{parts[0]}, {parts[1]}, India\"\n",
    "        elif len(parts) == 1:\n",
    "            cleaned = f\"{parts[0]}, India\"\n",
    "        else:\n",
    "            cleaned = str(location)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def geocode_location(self, location):\n",
    "        \"\"\"\n",
    "        Convertit un nom de lieu en coordonn√©es (lat, lon)\n",
    "        Utilise un cache pour √©viter les requ√™tes r√©p√©t√©es\n",
    "        \"\"\"\n",
    "        if pd.isna(location):\n",
    "            return None, None\n",
    "        \n",
    "        # V√©rifier le cache\n",
    "        if location in self.location_cache:\n",
    "            return self.location_cache[location]\n",
    "        \n",
    "        # Nettoyer la localisation\n",
    "        cleaned_location = self.clean_location_name(location)\n",
    "        \n",
    "        try:\n",
    "            loc_result = self.geolocator.geocode(cleaned_location, timeout=10)\n",
    "            \n",
    "            if loc_result:\n",
    "                lat, lon = loc_result.latitude, loc_result.longitude\n",
    "                self.location_cache[location] = (lat, lon)\n",
    "                \n",
    "                # Rate limiting pour Nominatim (max 1 req/sec)\n",
    "                time.sleep(1.1)\n",
    "                \n",
    "                return lat, lon\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è G√©ocodage √©chou√© pour: {cleaned_location}\")\n",
    "                self.location_cache[location] = (None, None)\n",
    "                return None, None\n",
    "                \n",
    "        except GeocoderTimedOut:\n",
    "            print(f\"‚è±Ô∏è Timeout pour: {cleaned_location}\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur g√©ocodage {cleaned_location}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def get_weather(self, lat, lon, date_str, hour):\n",
    "        \"\"\"\n",
    "        R√©cup√®re les conditions m√©t√©o pour une date/heure/lieu\n",
    "        date_str format: '2024-03-23'\n",
    "        \"\"\"\n",
    "        # Cr√©er cl√© de cache\n",
    "        cache_key = f\"{lat}_{lon}_{date_str}_{hour}\"\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            params = {\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"start_date\": date_str,\n",
    "                \"end_date\": date_str,\n",
    "                \"hourly\": [\"temperature_2m\", \"precipitation\", \"weathercode\", \"rain\"],\n",
    "                \"timezone\": \"auto\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.base_url, params=params, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            hourly = data['hourly']\n",
    "            \n",
    "            # V√©rifier que l'heure existe\n",
    "            if hour < len(hourly['weathercode']):\n",
    "                result = {\n",
    "                    'condition': self.categorize_weather(hourly['weathercode'][hour]),\n",
    "                    'temperature': hourly['temperature_2m'][hour],\n",
    "                    'precipitation': hourly['precipitation'][hour],\n",
    "                    'rain': hourly['rain'][hour],\n",
    "                    'weather_code': hourly['weathercode'][hour]\n",
    "                }\n",
    "            else:\n",
    "                result = None\n",
    "            \n",
    "            self.cache[cache_key] = result\n",
    "            time.sleep(0.05)  # Rate limiting l√©ger\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur API m√©t√©o: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def enrich_dataset(self, df, sample_size=None, use_drop_location=False):\n",
    "        \"\"\"\n",
    "        Enrichit le dataset avec les donn√©es m√©t√©o\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Dataset Uber original\n",
    "        sample_size : int, optional\n",
    "            Nombre de lignes √† traiter (None = tout)\n",
    "        use_drop_location : bool\n",
    "            Si True, utilise Drop Location au lieu de Pickup Location\n",
    "        \"\"\"\n",
    "        print(\"üöÄ D√©but de l'enrichissement m√©t√©o du dataset Uber\")\n",
    "        print(f\"üìä Taille du dataset: {len(df)} lignes\")\n",
    "        \n",
    "        # Cr√©er une copie\n",
    "        df_work = df.copy()\n",
    "        \n",
    "        # √âchantillonner si demand√©\n",
    "        if sample_size:\n",
    "            df_work = df_work.sample(n=min(sample_size, len(df_work)), random_state=42)\n",
    "            print(f\"üé≤ √âchantillon de {len(df_work)} lignes\")\n",
    "        \n",
    "        # Combiner Date et Time\n",
    "        print(\"\\nüìÖ Parsing des dates et heures...\")\n",
    "        df_work['datetime'] = pd.to_datetime(df_work['Date'] + ' ' + df_work['Time'])\n",
    "        df_work['date_only'] = df_work['datetime'].dt.date.astype(str)\n",
    "        df_work['hour'] = df_work['datetime'].dt.hour\n",
    "        \n",
    "        # Choisir la colonne de localisation\n",
    "        location_col = 'Drop Location' if use_drop_location else 'Pickup Location'\n",
    "        print(f\"üìç Utilisation de: {location_col}\")\n",
    "        \n",
    "        # G√©ocoder les localisations UNIQUES\n",
    "        print(f\"\\nüó∫Ô∏è G√©ocodage des localisations uniques...\")\n",
    "        unique_locations = df_work[location_col].dropna().unique()\n",
    "        print(f\"   {len(unique_locations)} localisations uniques √† g√©ocoder\")\n",
    "        \n",
    "        location_coords = {}\n",
    "        for i, loc in enumerate(unique_locations, 1):\n",
    "            lat, lon = self.geocode_location(loc)\n",
    "            location_coords[loc] = (lat, lon)\n",
    "            \n",
    "            if i % 5 == 0 or i == len(unique_locations):\n",
    "                print(f\"   Progression: {i}/{len(unique_locations)} ({i/len(unique_locations)*100:.1f}%)\")\n",
    "        \n",
    "        # Ajouter les coordonn√©es au DataFrame\n",
    "        print(\"\\nüéØ Application des coordonn√©es au dataset...\")\n",
    "        df_work['latitude'] = df_work[location_col].map(lambda x: location_coords.get(x, (None, None))[0])\n",
    "        df_work['longitude'] = df_work[location_col].map(lambda x: location_coords.get(x, (None, None))[1])\n",
    "        \n",
    "        # Statistiques g√©ocodage\n",
    "        geocoded = df_work['latitude'].notna().sum()\n",
    "        print(f\"   ‚úÖ {geocoded}/{len(df_work)} lignes g√©ocod√©es ({geocoded/len(df_work)*100:.1f}%)\")\n",
    "        \n",
    "        # R√©cup√©rer les donn√©es m√©t√©o\n",
    "        print(\"\\nüå¶Ô∏è R√©cup√©ration des donn√©es m√©t√©o...\")\n",
    "        weather_results = []\n",
    "        \n",
    "        total = len(df_work)\n",
    "        for idx, row in df_work.iterrows():\n",
    "            if pd.notna(row['latitude']) and pd.notna(row['longitude']):\n",
    "                weather = self.get_weather(\n",
    "                    row['latitude'],\n",
    "                    row['longitude'],\n",
    "                    row['date_only'],\n",
    "                    row['hour']\n",
    "                )\n",
    "                \n",
    "                if weather:\n",
    "                    weather_results.append(weather)\n",
    "                else:\n",
    "                    weather_results.append(self._empty_weather())\n",
    "            else:\n",
    "                weather_results.append(self._empty_weather())\n",
    "            \n",
    "            # Progress\n",
    "            if len(weather_results) % 50 == 0:\n",
    "                print(f\"   Trait√©: {len(weather_results)}/{total} ({len(weather_results)/total*100:.1f}%)\")\n",
    "        \n",
    "        # Cr√©er DataFrame m√©t√©o\n",
    "        weather_df = pd.DataFrame(weather_results)\n",
    "        \n",
    "        # Ajouter au dataset original\n",
    "        result_df = pd.concat([df_work.reset_index(drop=True), weather_df], axis=1)\n",
    "        \n",
    "        # Statistiques finales\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ ENRICHISSEMENT TERMIN√â !\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nüìä Distribution des conditions m√©t√©o:\")\n",
    "        print(result_df['condition'].value_counts())\n",
    "        print(f\"\\nüå°Ô∏è Temp√©rature moyenne: {result_df['temperature'].mean():.1f}¬∞C\")\n",
    "        print(f\"üíß Pr√©cipitations moyennes: {result_df['precipitation'].mean():.2f}mm\")\n",
    "        \n",
    "        # Features suppl√©mentaires\n",
    "        result_df = self._add_weather_features(result_df)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def _empty_weather(self):\n",
    "        \"\"\"Retourne un dict m√©t√©o vide\"\"\"\n",
    "        return {\n",
    "            'condition': 'Unknown',\n",
    "            'temperature': None,\n",
    "            'precipitation': None,\n",
    "            'rain': None,\n",
    "            'weather_code': None\n",
    "        }\n",
    "    \n",
    "    def _add_weather_features(self, df):\n",
    "        \"\"\"Ajoute des features ML d√©riv√©es de la m√©t√©o\"\"\"\n",
    "        print(\"\\nüîß Cr√©ation de features suppl√©mentaires...\")\n",
    "        \n",
    "        # 1. Pluie binaire\n",
    "        df['is_rainy'] = df['condition'].isin(['Light Rain', 'Moderate Rain', \n",
    "                                                 'Heavy Rain', 'Rain Showers', \n",
    "                                                 'Thunderstorm', 'Heavy Thunderstorm']).astype(int)\n",
    "        \n",
    "        # 2. M√©t√©o extr√™me\n",
    "        df['extreme_weather'] = df['condition'].isin(['Heavy Rain', 'Thunderstorm', \n",
    "                                                        'Heavy Thunderstorm', 'Snow']).astype(int)\n",
    "        \n",
    "        # 3. Cat√©gorie temp√©rature\n",
    "        df['temp_category'] = pd.cut(df['temperature'], \n",
    "                                      bins=[-np.inf, 15, 25, 35, np.inf],\n",
    "                                      labels=['Cold', 'Moderate', 'Warm', 'Hot'])\n",
    "        \n",
    "        # 4. Niveau pr√©cipitation\n",
    "        df['precipitation_level'] = pd.cut(df['precipitation'],\n",
    "                                            bins=[-0.1, 0.1, 2.5, 10, np.inf],\n",
    "                                            labels=['None', 'Light', 'Moderate', 'Heavy'])\n",
    "        \n",
    "        print(\"   ‚úÖ Features cr√©√©es: is_rainy, extreme_weather, temp_category, precipitation_level\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# ============== UTILISATION PRATIQUE ==============\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale d'utilisation\"\"\"\n",
    "    \n",
    "    # 1. Charger le dataset\n",
    "    print(\"üìÇ Chargement du dataset...\")\n",
    "    df = pd.read_csv('uber_rides_data.csv')\n",
    "    \n",
    "    print(f\"‚úÖ Dataset charg√©: {len(df)} lignes, {len(df.columns)} colonnes\")\n",
    "    print(\"\\nüîç Aper√ßu des donn√©es:\")\n",
    "    print(df[['Date', 'Time', 'Pickup Location', 'Booking Value']].head())\n",
    "    \n",
    "    # 2. Cr√©er l'enrichisseur\n",
    "    enricher = WeatherEnricher()\n",
    "    \n",
    "    # 3. OPTION A: Tester sur un √©chantillon (RECOMMAND√â POUR D√âBUTER)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1: TEST SUR √âCHANTILLON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_sample = enricher.enrich_dataset(df, sample_size=100)\n",
    "    df_sample.to_csv('uber_with_weather_SAMPLE.csv', index=False)\n",
    "    print(f\"\\nüíæ √âchantillon sauvegard√©: uber_with_weather_SAMPLE.csv\")\n",
    "    \n",
    "    # 4. V√©rifier les r√©sultats\n",
    "    print(\"\\nüìà Analyse de l'impact m√©t√©o sur les prix:\")\n",
    "    if 'Booking Value' in df_sample.columns:\n",
    "        avg_price_rain = df_sample[df_sample['is_rainy']==1]['Booking Value'].mean()\n",
    "        avg_price_clear = df_sample[df_sample['is_rainy']==0]['Booking Value'].mean()\n",
    "        \n",
    "        if pd.notna(avg_price_rain) and pd.notna(avg_price_clear):\n",
    "            increase = ((avg_price_rain - avg_price_clear) / avg_price_clear) * 100\n",
    "            print(f\"   üí∞ Prix moyen (pluie): ‚Çπ{avg_price_rain:.2f}\")\n",
    "            print(f\"   ‚òÄÔ∏è Prix moyen (beau): ‚Çπ{avg_price_clear:.2f}\")\n",
    "            print(f\"   üìä Augmentation: {increase:.1f}%\")\n",
    "    \n",
    "    # 5. OPTION B: Traiter tout le dataset (√† faire apr√®s validation)\n",
    "    # D√âCOMMENTER SI L'√âCHANTILLON EST BON\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 2: TRAITEMENT COMPLET (cela peut prendre du temps)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    response = input(\"Voulez-vous continuer avec le dataset complet? (oui/non): \")\n",
    "    if response.lower() == 'oui':\n",
    "        df_full = enricher.enrich_dataset(df, sample_size=None)\n",
    "        df_full.to_csv('uber_with_weather_FULL.csv', index=False)\n",
    "        print(f\"\\nüíæ Dataset complet sauvegard√©: uber_with_weather_FULL.csv\")\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ TERMIN√â - Pr√™t pour votre ML model!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_sample\n",
    "\n",
    "\n",
    "# Lancer le script\n",
    "if __name__ == \"__main__\":\n",
    "    df_enriched = main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36d6bbf7a52f7c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
